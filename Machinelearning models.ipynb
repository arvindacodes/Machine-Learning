{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Loading the train and test Data-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn import impute\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train and test datasets  from UCI website\n",
    "Original_train_dataset = pd.read_csv('adult.txt', delimiter=',\\s',na_values=\"?\",engine='python',header= None)\n",
    "Original_test_dataset = pd.read_csv('adult_test.txt', delimiter= ',\\s',na_values=\"?\",engine='python',header=1)\n",
    "\n",
    "#printing only 5 rows from train and test dataset\n",
    "print(Original_train_dataset.head(5))\n",
    "print(Original_test_dataset.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there are no column names, adding them manually for convenient understanding.\n",
    "\n",
    "Original_train_dataset.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\n",
    "\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"income\"]\n",
    "\n",
    "# it was seen that the test data had a \"full stop\" at end of each line. hence removing it here \n",
    "Original_test_dataset.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\n",
    "\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"income\"]\n",
    "Original_test_dataset['income'].replace(regex=True,inplace=True,to_replace=r'\\.',value=r'')\n",
    "\n",
    "#printing only one row to see if column names are proper:\n",
    "\n",
    "print(\"train dataset is:\",Original_train_dataset.head(1))\n",
    "print(\"test dataset is :\",Original_test_dataset.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here am joining both the train and test datafranes together. this is to complete pre-processing of data toghether.\n",
    "adult_df = pd.concat([Original_train_dataset,Original_test_dataset],sort=False)\n",
    "print(adult_df.head(5))\n",
    "print(adult_df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of adult_df which has both the train and test data.\n",
    "print(\"shape of the combined dataset is:\",\"\\n\",adult_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2:  Visualization and Pre-processing of data.\n",
    "## Section 2.1 : Visualization of Train and test Features.\n",
    "### Section 2.1.1 :  Checking the infomation of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the information of this dataframe to see if pre-processing is necessary \n",
    "print(adult_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.2:Checking the summary statistics for both Numerical and Categorical features seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the summary statistics of the adult_df \n",
    "adult_df.describe() # this will give the statistics of only the Numerical Features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above statistics for numerical features it is clearly seen that, there are no missing values in any of the features. All the features have total of 48841 rows with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df.describe(include = ['O'])# Gives statistics of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics of Categorical features shows that, there are few missing values in feature workclass, occupation and Native country. Next step is to find out how many values are missing in each of the features. But before finding the number of missing values in each feature, performing a simple comparison of each categorical feature with Income class to see if we can drop any of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.3: Comparing the \"Income\" label class with other categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using seaborn, am trying to count the occurances of each values.\n",
    "print(\"Graph1:\",sns.countplot(y='occupation', hue='income', data=adult_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph 2:\",sns.countplot(y='education', hue='income', data=adult_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph 3:\",sns.countplot(y='sex', hue='income', data=adult_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph 4:\",sns.countplot(y='race', hue='income', data=adult_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph 5:\",sns.countplot(y='workclass', hue='income', data=adult_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph 6:\",sns.countplot(y='native-country', hue='income', data=adult_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph 7:\",sns.countplot(y='relationship', hue='income', data=adult_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph 8:\",sns.countplot(y='marital-status', hue='income', data=adult_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference from the all the above 8 graphs are as followed:                                                                 1. Around 90% of values in native country belongs to United States\n",
    "    2. Around 85% of values in race feature are White.\n",
    "    3. Almost all the categorical features have relationship with the label class \"INCOME\". Hence we cannot decide to drop any of the features now. This leads us to carry out the process of feature  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.4: Checking for missing value count in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total number of nan present before imputations\" '\\n',adult_df.isna().sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Preprocessing of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the complete process of preprocessing and preparing the data for feeding to classifer models will be carried out extensively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2.1: Imputation using Simple imputer\n",
    "Here am using simple imputer funtion to impute the missing values in the 3 categorical feature obtained in section 2.1.4. The missing values will be replaced by most frequently available values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputeclass = impute.SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "adult_df[[\"workclass\"]] = imputeclass.fit_transform(adult_df[[\"workclass\"]])\n",
    "adult_df[[\"occupation\"]] = imputeclass.fit_transform(adult_df[[\"occupation\"]])\n",
    "adult_df[[\"native-country\"]] = imputeclass.fit_transform(adult_df[[\"native-country\"]])\n",
    "print(\"total number of nan present after imputations\" '\\n',adult_df.isna().sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2.2 : Handling the label class [\"Income\"] seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here am replacing >50K with 1 and <50K with 0\n",
    "print(adult_df['income'].value_counts())\n",
    "# print(adult_df['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df['income'] = [0 if x == '<=50K' else 1 for x in adult_df['income']]\n",
    "print(adult_df['income'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2.3: Using one-hot encoder, changing all the categorical features to numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying one-hot encoder to each categorical feature one by one\n",
    "\n",
    "Encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "adult_df['workclass'] = Encoder.fit_transform(adult_df[['workclass']])\n",
    "# train_feature[['education','marital-status']] = Encoder.fit_transform(train_feature[['education','marital-status']])\n",
    "adult_df['education'] = Encoder.fit_transform(adult_df[['education']])\n",
    "adult_df['marital-status'] = Encoder.fit_transform(adult_df[['marital-status']])\n",
    "adult_df['occupation'] = Encoder.fit_transform(adult_df[['occupation']])\n",
    "adult_df['race'] = Encoder.fit_transform(adult_df[['race']])\n",
    "adult_df['relationship'] = Encoder.fit_transform(adult_df[['relationship']])\n",
    "adult_df['sex'] = Encoder.fit_transform(adult_df[['sex']])\n",
    "\n",
    "\n",
    "# adult_df['native-country'] = Encoder.fit_transform(adult_df[['native-country']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the information of the adult_df after pre-processing\n",
    "print(adult_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the native-country feature and converting them seperately.\n",
    "It can be clearly seen now from the information that all the categorical features are transformed to numerical feature except the feature \"Native-country\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for unique values in column native-country\n",
    "uniq = adult_df['native-country'].unique()\n",
    "print(uniq)\n",
    "print(len(uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all the countries which are not UnitedStates to \"OTHERS\"\n",
    "adult_df['native-country']= ['United-States' if x == 'United-States' else 'Others' for x in adult_df['native-country']] \n",
    "uniq = adult_df['native-country'].unique()\n",
    "print(uniq)\n",
    "print(len(uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the categorical native-country feature is converted to numerical feature.\n",
    "adult_df['native-country'] = Encoder.fit_transform(adult_df[['native-country']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adult_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, i have converted all the categorical feature data to numerical feature data. Hence the data is now ready for feeding into the classifer models to be built in next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3: Splitting data and building classifier Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the preprocessed data will be splitted to Train feature, Test features and  Train label, Test label. \n",
    "These datasets will be then used to build various classifier models such as SVM, Naive Bayes, Decision Tree, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3.1: Splitting of data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the adult dataframe into feature and \n",
    "adultdf_feature =adult_df.drop('income',axis = 1 )\n",
    "adultdf_label = adult_df['income']\n",
    "print(adultdf_feature.head(5))\n",
    "print(adultdf_label.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the train_test_split function, dividing the data into trainfeature, trainlabel and testfeature, testlabel\n",
    "train_feature,test_feature,train_label,test_label = train_test_split(adultdf_feature, adultdf_label, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"The shape of train feature is :\", train_feature.shape)\n",
    "print(\"The shape of train label is :\",train_label.shape)\n",
    "print(\"The shape of test feature is :\",test_feature.shape)\n",
    "print(\"The shape of test label is :\",test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Building Classifier Models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. K-nearest neighbour model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN model with all the features\n",
    "knn = KNeighborsClassifier(n_neighbors=28)\n",
    "knn.fit(train_feature, train_label)\n",
    "knn_pred = knn.predict(test_feature)\n",
    "# print(knn_pred)\n",
    "knnaccuracy = metrics.accuracy_score(test_label,knn_pred)\n",
    "print (\"KNN Accuracy \", knnaccuracy)\n",
    "cf_mat = confusion_matrix(test_label,knn_pred)\n",
    "print(\"confusion matrix is :\",cf_mat)\n",
    "print(\"classification report is  : \" '\\n',classification_report(test_label, knn_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gaussian NAive Bayes Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes model with all the features:\n",
    "nBayes = GaussianNB()\n",
    "nBayes = nBayes.fit(train_feature, train_label)\n",
    "Bayes_pred = nBayes.predict(test_feature)\n",
    "Bayesaccuracy = metrics.accuracy_score(test_label,Bayes_pred)\n",
    "print (\"Naive Bayes Accuracy \", Bayesaccuracy)\n",
    "cf_mat = confusion_matrix(test_label,Bayes_pred)\n",
    "print(\"confusion matrix is :\",cf_mat)\n",
    "print(\"F1_Score : \",f1_score(test_label, Bayes_pred))\n",
    "print(\"classification report is  : \" '\\n',classification_report(test_label, Bayes_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SVM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc = svc.fit(train_feature, train_label)\n",
    "svc_pred = svc.predict(test_feature)\n",
    "svcaccuracy = metrics.accuracy_score(test_label,svc_pred)\n",
    "print (\"SVM Accuracy \", svcaccuracy)\n",
    "cf_mat = confusion_matrix(test_label,svc_pred)\n",
    "print(\"confusion matrix is :\",cf_mat)\n",
    "print(\"F1_Score : \",f1_score(test_label, svc_pred))\n",
    "print(\"classification report is  : \" '\\n',classification_report(test_label, svc_pred))\n",
    "\n",
    "# print(svc_pred)\n",
    "# print(\"F1_Score : \",f1_score(test_label, svc_pred))\n",
    "# conf_matrix_svc = confusion_matrix(test_label,svc_pred)\n",
    "# print(conf_matrix_svc)\n",
    "# print(classification_report(test_label,svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Decision tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision Model with all the features \n",
    "decTree = DecisionTreeClassifier()\n",
    "decTree = decTree.fit(train_feature, train_label)\n",
    "decTree_pred = decTree.predict(test_feature)\n",
    "decTreeaccuracy = metrics.accuracy_score(test_label,decTree_pred)\n",
    "print (\"Decision Tree Accuracy \", decTreeaccuracy)\n",
    "cf_mat = confusion_matrix(test_label,decTree_pred)\n",
    "print(\"confusion matrix is :\",cf_mat)\n",
    "print(\"F1_Score : \",f1_score(test_label, decTree_pred))\n",
    "print(\"classification report is  : \" '\\n',classification_report(test_label, decTree_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Random forest classifer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifer with all the features\n",
    "RFC=RandomForestClassifier()\n",
    "RFC = RFC.fit(train_feature, train_label)\n",
    "RFC_pred = RFC.predict(test_feature)\n",
    "print(RFC_pred)\n",
    "RFCaccuracy = metrics.accuracy_score(test_label,RFC_pred)\n",
    "print (\"Random Forest Accuracy \", RFCaccuracy)\n",
    "conf_matrix_RFC = confusion_matrix(test_label,RFC_pred)\n",
    "print(conf_matrix_RFC)\n",
    "print(\"F1_Score : \",f1_score(test_label, RFC_pred))\n",
    "print(\"classification report is  : \" '\\n',classification_report(test_label, RFC_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It is clearly seen from the above section that all the classifer models have accuracy between the range of 75% to 85% . This is good but only accuracy cannot be considered for choosing the best classifier model. Therefore it is also infered that the F1_score is very poor for almost all the models. Hence  we will employ feature selection techniques to choose the appropriate features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3:  Different feature selection techniques and Feature encoding techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1 Feature selection techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.1.1: Univariate Selection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature  = SelectKBest(score_func=chi2, k=10)\n",
    "fit = best_feature.fit(adultdf_feature, adultdf_label)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(adultdf_feature.columns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(14,'Score'))  #print 10 best features\n",
    "\n",
    "\n",
    "# # np.set_printoptions(precision=3)\n",
    "# # print((fit.scores_)/10000000000)\n",
    "# # features = fit.transform(adultdf_feature)\n",
    "# # print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 : Tree Based feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=250, random_state=0)\n",
    "forest.fit(adultdf_feature, adultdf_label)\n",
    "# importances contains the feature importance value for each metric\n",
    "importances_train = forest.feature_importances_\n",
    "print(importances_train)\n",
    "# argsort returns the indices that will sort the original array\n",
    "sortedIndices = np.argsort(importances_train)\n",
    "print(\"The Indices are \"'\\n',sortedIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 10)\n",
    "fit = rfe.fit(adultdf_feature, adultdf_label)\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % (fit.support_))\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therfore it is clearly seen from the above three feature selection technique that  features ‘education’, ‘fnlwgt’  are common and hence can be removed from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Section 3.2 : Create new dataset by dropping the weakest feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the weakest features from the train_feature, test feature datasets.\n",
    "\n",
    "train_feature_new = train_feature.drop(['education','fnlwgt'],axis = 1)\n",
    "test_feature_new = test_feature.drop(['education','fnlwgt'],axis = 1)\n",
    "print(\"The shape of train feature is :\", train_feature_new.shape)\n",
    "print(\"The shape of train label is :\",train_label.shape)\n",
    "print(\"The shape of test feature is :\",test_feature_new.shape)\n",
    "print(\"The shape of test label is :\",test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section  3.3 : Create classifer models for new dataset, that is weakest features are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created datasets will be passed to the classifiers created in section 2.3.2 as it will be easy to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knn Model with weakest features dropped from the dataset\n",
    "print(\"==================================================================\")\n",
    "knn_new = KNeighborsClassifier(n_neighbors=28)\n",
    "knn_new.fit(train_feature_new, train_label)\n",
    "knn_pred_new = knn_new.predict(test_feature_new)\n",
    "# print(knn_pred)\n",
    "knnaccuracy_new = metrics.accuracy_score(test_label,knn_pred_new)\n",
    "print (\"New KNN Accuracy \", knnaccuracy_new)\n",
    "cf_mat_new = confusion_matrix(test_label,knn_pred_new)\n",
    "print(\"New confusion matrix is :\",cf_mat_new)\n",
    "print(\"New classification report is  : \" '\\n',classification_report(test_label, knn_pred_new))\n",
    "\n",
    "\n",
    "#==================================================\n",
    "\n",
    "#Naive Bayes with weakest features dropped \n",
    "\n",
    "nBayes_new = GaussianNB()\n",
    "nBayes_new = nBayes_new.fit(train_feature_new, train_label)\n",
    "Bayes_pred_new = nBayes_new.predict(test_feature_new)\n",
    "Bayesaccuracy_new = metrics.accuracy_score(test_label,Bayes_pred_new)\n",
    "print (\"Naive Bayes Accuracy \", Bayesaccuracy_new)\n",
    "cf_mat_new = confusion_matrix(test_label,Bayes_pred_new)\n",
    "print(\"confusion matrix is :\",cf_mat_new)\n",
    "print(\"F1_Score : \",f1_score(test_label, Bayes_pred_new))\n",
    "print(\"classification report is  : \" '\\n',classification_report(test_label, Bayes_pred_new))\n",
    "\n",
    "#==========================================================\n",
    "\n",
    "#decision model with weakest features dropped from dataset \n",
    "decTree_new = DecisionTreeClassifier()\n",
    "decTree_new = decTree_new.fit(train_feature_new, train_label)\n",
    "decTree_pred_new = decTree_new.predict(test_feature_new)\n",
    "decTreeaccuracy_new = metrics.accuracy_score(test_label,decTree_pred_new)\n",
    "print (\"New Decision Tree Accuracy \", decTreeaccuracy_new)\n",
    "cf_mat_new = confusion_matrix(test_label,decTree_pred_new)\n",
    "print(\"New confusion matrix is :\",cf_mat_new)\n",
    "print(\"New F1_Score : \",f1_score(test_label, decTree_pred_new))\n",
    "print(\"New classification report is  : \" '\\n',classification_report(test_label, decTree_pred_new))\n",
    "\n",
    "#==========================================================================================\n",
    "\n",
    "#Random forest classifer with weakest feature dropped.\n",
    "\n",
    "RFC_new=RandomForestClassifier()\n",
    "RFC_new = RFC_new.fit(train_feature_new, train_label)\n",
    "RFC_pred_new = RFC_new.predict(test_feature_new)\n",
    "print(RFC_pred_new)\n",
    "RFCaccuracy_new = metrics.accuracy_score(test_label,RFC_pred_new)\n",
    "print (\"New Random Forest Accuracy \", RFCaccuracy_new)\n",
    "conf_matrix_RFC_new = confusion_matrix(test_label,RFC_pred_new)\n",
    "print(conf_matrix_RFC_new)\n",
    "print(\"New F1_Score : \",f1_score(test_label, RFC_pred_new))\n",
    "print(\"New classification report is  : \" '\\n',classification_report(test_label, RFC_pred_new))\n",
    "\n",
    "svc_new = SVC()\n",
    "svc_new = svc_new.fit(train_feature_new, train_label)\n",
    "svc_pred_new = svc_new.predict(test_feature_new)\n",
    "svcaccuracy_new = metrics.accuracy_score(test_label,svc_pred_new)\n",
    "print (\"New SVM Accuracy \", svcaccuracy_new)\n",
    "cf_mat = confusion_matrix(test_label,svc_pred_new)\n",
    "print(\"New confusion matrix is :\",cf_mat_new)\n",
    "print(\"New F1_Score : \",f1_score(test_label, svc_pred_new))\n",
    "print(\"New classification report is  : \" '\\n',classification_report(test_label, svc_pred_new))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hyper-parameter Optimization:¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross fold Validation using cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the weakest columns from original dataframe so that it can be used in hyper parameter optimization.\n",
    "\n",
    "#splitting the adult dataframe into feature and \n",
    "adultdf_featurenew =adult_df.drop(['income','education','fnlwgt'],axis = 1 )\n",
    "adultdf_labelnew = adult_df['income']\n",
    "print(adultdf_featurenew.shape)\n",
    "print(adultdf_labelnew.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier().fit(train_feature_new, train_label) #Initialize with whatever parameters you want to\n",
    "\n",
    "# 10-Fold Cross validation\n",
    "print (np.mean(cross_val_score(clf, adultdf_feature, adultdf_label, cv=10)))\n",
    "print (\"F1_score is :\",np.mean(cross_val_score(clf, adultdf_feature, adultdf_label, cv=10,scoring='f1_macro')))\n",
    "# clf.score(test_feature,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "param_grid = { \n",
    "    'n_estimators': [200],\n",
    "    'max_features': ['auto'], #selects all the features \n",
    "    'max_depth' : [10],\n",
    "    'criterion' :['entropy']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(train_feature_new, train_label)\n",
    "CVRFC_pred = CV_rfc.predict(test_feature_new)\n",
    "print(CVRFC_pred)\n",
    "CVRFCaccuracy = metrics.accuracy_score(test_label,CVRFC_pred)\n",
    "print(CVRFCaccuracy)\n",
    "print(\"F1_Score : \",f1_score(test_label, CVRFC_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
